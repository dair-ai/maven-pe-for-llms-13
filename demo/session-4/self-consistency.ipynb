{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Paper Tagger & LLM Evaluator\n",
    "\n",
    "Here what's provided in the notebook:\n",
    "- Compares zero-shot, few-shot, CoT, and self-consistency\n",
    "- Evaluates using an LLM evaluator using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# update or install the necessary libraries\n",
    "!pip install --upgrade openai\n",
    "!pip install --upgrade langchain\n",
    "!pip install --upgrade python-dotenv\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elvissaravia/opt/miniconda3/envs/pe-for-llms/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# set the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# create a new LLM\n",
    "from langchain.llms import OpenAI\n",
    "llm  = OpenAI()\n",
    "\n",
    "def print_markdown(text):\n",
    "    \"\"\"Prints text as markdown\"\"\"\n",
    "    IPython.display.display(IPython.display.Markdown(text))\n",
    "\n",
    "# load json data at path: data/article-tags.json into a dataframe\n",
    "with open('../data/article-tags.json') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "with open('../data/few_shot.json') as f:\n",
    "    few_shot_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer example selector\n",
    "\n",
    "class CustomExampleSelector(BaseExampleSelector):\n",
    "    \n",
    "    def __init__(self, examples: List[Dict[str, str]]):\n",
    "        self.examples = examples\n",
    "    \n",
    "    def add_example(self, example: Dict[str, str]) -> None:\n",
    "        \"\"\"Add new example to store for a key.\"\"\"\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, size) -> List[dict]:\n",
    "        \"\"\"Select which examples to use based on the inputs.\"\"\"\n",
    "        return np.random.choice(self.examples, size=size, replace=False)\n",
    "    \n",
    "\n",
    "example_selector = CustomExampleSelector(few_shot_data)\n",
    "\n",
    "template = \"\"\"\n",
    "Abstract: {abstract}\n",
    "Tags: {tags}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"abstract\", \"tags\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples = list(example_selector.select_examples(3)),\n",
    "    example_prompt=prompt,\n",
    "    prefix = \"Your task is to extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\",\n",
    "    suffix = \"Abstract: {input}\\nTags:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\" \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot prompt\n",
    "zero_shot_template = \"\"\"\n",
    "Your task is extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\n",
    "\n",
    "Abstract: {abstract}\n",
    "Tags:\n",
    "\"\"\"\n",
    "\n",
    "zero_shot_prompt = PromptTemplate(\n",
    "    input_variables=[\"abstract\"],\n",
    "    template=zero_shot_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Consistency with Zero-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call as a batch to get separate responses from the model\n",
    "\n",
    "# zero-shot prompt\n",
    "self_consistency_template = \"\"\"\n",
    "Your task is extract model names from machine learning paper abstracts delimited by ```. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\n",
    "\n",
    "Let's think step by step: <steps>\n",
    "Abstract: ```{abstract}```\n",
    "Tags (should just output the model names in an array):\n",
    "\"\"\"\n",
    "\n",
    "self_consistency_prompt = PromptTemplate(\n",
    "    input_variables=[\"abstract\"],\n",
    "    template=self_consistency_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define chains\n",
    "\n",
    "zero_chain = LLMChain(llm=llm, prompt=zero_shot_prompt)\n",
    "few_shot_chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "self_consistency_chain = LLMChain(llm=llm, prompt=self_consistency_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the abstracts from val_data\n",
    "abstracts = [{\"input\": val_data[i][\"abstract\"]} for i in range(len(val_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot predictions\n",
    "few_shot_predictions = few_shot_chain.apply(abstracts)\n",
    "\n",
    "# zero-shot predictions\n",
    "zero_shot_predictions = zero_chain.apply(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \" ['Evol-Instruct', 'LLM', 'WizardLM', 'ChatGPT']\"},\n",
       " {'text': \" ['FLAN-T5', 'LoRA', 'AMR', 'Smatch']\"},\n",
       " {'text': \" ['AI', 'scientific knowledge', 'hypothesis machines']\"},\n",
       " {'text': \" ['PAXQA', 'QG', 'QA']\"},\n",
       " {'text': \" ['ChatGPT']\"},\n",
       " {'text': \" ['ViT', 'OpenCLIP']\"},\n",
       " {'text': \" ['SAM', 'IA', 'AIGC', 'Stable Diffusion']\"},\n",
       " {'text': \" ['Anything-3D', 'BLIP', 'Segment-Anything', 'text-to-image diffusion model', 'neural radiance field']\"},\n",
       " {'text': \" ['Chameleon', 'LLMs', 'GPT-4', 'ScienceQA', 'TabMWP', 'ChatGPT']\"},\n",
       " {'text': \" ['foundation models']\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"text\": '[\"LLM\", \"Evol-Instruct\", \"LLaMA\", \"WizardLM\", \"OpenAI ChatGPT\"]'}, {\"text\": '[\"FLAN-T5\", \"LoRA\"]'}, {\"text\": '[\"large language models\", \"generative AI\", \"hypothesis machines\"]'}, {\"text\": '[\"PAXQA\", \"lexically-constrained machine translation\", \"question generation (QG) model\", \"extractive QA models\", \"zero-shot\", \"synthetic data generation models\"]'}, {\"text\": '[\"ChatGPT\", \"NA\", \"NA\"]'}, {\"text\": '[\"ViT\", \"OpenCLIP\"]'}, {\"text\": '[\"Segment-Anything Model (SAM)\", \"Inpaint Anything (IA)\", \"AIGC models\", \"Stable Diffusion\"]'}, {\"text\": '[\"BLIP\", \"Segment-Anything\", \"text-to-image diffusion model\"]'}, {\"text\": '[\"GPT-4\", \"ChatGPT\"]'}, {\"text\": '[\"foundation models\"]'}]\n"
     ]
    }
   ],
   "source": [
    "# self-consistency predictions\n",
    "num_of_samples = 5\n",
    "len_of_abstracts = len(abstracts)\n",
    "output_format = \"\"\"[{\"text\": '[<model_names>]'}, {\"text\": '[<model_names>]'}, ...]\"\"\"\n",
    "\n",
    "# number of time to produce tags for each abstract\n",
    "self_consistency_samples = [self_consistency_chain.apply(val_data) for i in range(num_of_samples)]\n",
    "\n",
    "def get_model_names(samples):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are given an array of outputs (delimited by ####) generated by a model. \\n\\nThe outputs contain a list of model names the model generated for {len_of_abstracts} different abstracts. There are {num_of_samples} different samples in the data.\\n\\nYour task is to pick the most common and consistent output from the {num_of_samples} different samples, corresponding to each of the 10 different abstracts. \\n\\nOutput format should be {output_format}\"\"\".format(len_of_abstracts=len_of_abstracts, num_of_samples=num_of_samples, output_format=output_format)\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"####\\n{samples}\\n####\"\"\".format(samples=self_consistency_samples)\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=256,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# get the model names\n",
    "self_consistency_predictions = get_model_names(self_consistency_samples)\n",
    "\n",
    "print(self_consistency_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert self_consistency_predictions to list of objects\n",
    "\n",
    "final_self_consistency_predictions = eval(self_consistency_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation chain from LangChain (using an LLM to evaluate)\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "zero_shot_graded_outputs = eval_chain.evaluate(val_data, zero_shot_predictions, question_key=\"abstract\", prediction_key=\"text\", answer_key=\"tags\")\n",
    "\n",
    "fw_graded_ouputs = eval_chain.evaluate(val_data, few_shot_predictions, question_key=\"abstract\", prediction_key=\"text\", answer_key=\"tags\")\n",
    "\n",
    "self_consistency_graded_outputs = eval_chain.evaluate(val_data, final_self_consistency_predictions, question_key=\"abstract\", prediction_key=\"text\", answer_key=\"tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'results': ' CORRECT'},\n",
       " {'results': ' INCORRECT'},\n",
       " {'results': ' INCORRECT'},\n",
       " {'results': ' CORRECT'},\n",
       " {'results': ' CORRECT'},\n",
       " {'results': ' CORRECT'},\n",
       " {'results': ' CORRECT'},\n",
       " {'results': ' CORRECT'},\n",
       " {'results': ' CORRECT'},\n",
       " {'results': ' CORRECT'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Real Answer: ['LLaMA', 'ChatGPT', 'WizardLM']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"WizardLM\", \"LLM\", \"Evol-Instruct\", \"LLaMA\", \"OpenAI ChatGPT\"]\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['Evol-Instruct', 'LLM', 'WizardLM', 'ChatGPT']\n",
      "Few-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"LLM\", \"Evol-Instruct\", \"LLaMA\", \"WizardLM\", \"OpenAI ChatGPT\"]\n",
      "Self-consistency Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Real Answer: ['FLAN-T5', 'FLAN']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"FLAN\", \"FLAN-T5\", \"AMR\", \"UD\", \"SRL\", \"AMR2.0\", \"AMR3.0\", \"BioAMR\", \"LoRA\"]\n",
      "Zero-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['FLAN-T5', 'LoRA', 'AMR', 'Smatch']\n",
      "Few-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"FLAN-T5\", \"LoRA\"]\n",
      "Self-consistency Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 2:\n",
      "Real Answer: ['NA']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"large language models\", \"generative AI\", \"hypothesis machines\", \"automated experimentation\", \"adversarial peer reviews\"]\n",
      "Zero-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['AI', 'scientific knowledge', 'hypothesis machines']\n",
      "Few-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"large language models\", \"generative AI\", \"hypothesis machines\"]\n",
      "Self-consistency Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 3:\n",
      "Real Answer: ['PAXQA']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"PAXQA\", \"Question Generation\", \"Lexically-Constrained Machine Translation\", \"Zero-Shot Methods\", \"Extractive QA Models\", \"Cross-Lingual QA\", \"Indirect Supervision\", \"Annotation Projection\", \"Parallel Corpora\", \"Question Answering\", \"Fine-Tuned Models\", \"Synthetic Data Generation\", \"Automatic Word Alignments\", \"Non-English Questions\"]\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['PAXQA', 'QG', 'QA']\n",
      "Few-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"PAXQA\", \"lexically-constrained machine translation\", \"question generation (QG) model\", \"extractive QA models\", \"zero-shot\", \"synthetic data generation models\"]\n",
      "Self-consistency Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 4:\n",
      "Real Answer: ['ChatGPT']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"ChatGPT\", \"NA\"]\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['ChatGPT']\n",
      "Few-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"ChatGPT\", \"NA\", \"NA\"]\n",
      "Self-consistency Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 5:\n",
      "Real Answer: ['OpenCLIP', 'ViT']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"ViT\", \"OpenCLIP\"]\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['ViT', 'OpenCLIP']\n",
      "Few-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"ViT\", \"OpenCLIP\"]\n",
      "Self-consistency Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 6:\n",
      "Real Answer: ['SAM', 'IA']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"Segment-Anything Model (SAM)\", \"Inpaint Anything (IA)\", \"AIGC models\", \"Stable Diffusion\"]\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['SAM', 'IA', 'AIGC', 'Stable Diffusion']\n",
      "Few-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"Segment-Anything Model (SAM)\", \"Inpaint Anything (IA)\", \"AIGC models\", \"Stable Diffusion\"]\n",
      "Self-consistency Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 7:\n",
      "Real Answer: ['Anything-3D', 'BLIP', 'Segment-Anything']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: array([\"Anything-3D\", \"Segment-Anything\", \"BLIP\", \"text-to-image diffusion model\", \"neural radiance field\", \"3D reconstruction\"])\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['Anything-3D', 'BLIP', 'Segment-Anything', 'text-to-image diffusion model', 'neural radiance field']\n",
      "Few-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"BLIP\", \"Segment-Anything\", \"text-to-image diffusion model\"]\n",
      "Self-consistency Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 8:\n",
      "Real Answer: ['Chameleon', 'GPT-4', 'ChatGPT']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"Chameleon\", \"GPT-4\", \"ChatGPT\"]\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['Chameleon', 'LLMs', 'GPT-4', 'ScienceQA', 'TabMWP', 'ChatGPT']\n",
      "Few-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"GPT-4\", \"ChatGPT\"]\n",
      "Self-consistency Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 9:\n",
      "Real Answer: ['NA']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"foundation models\", \"AI systems\", \"tool learning\", \"specialized tools\", \"enhanced accuracy\", \"efficiency\", \"automation\", \"cognitive origins\", \"paradigm shift\", \"complementary roles\", \"tool-augmented learning\", \"tool-oriented learning\", \"general tool learning framework\", \"user instruction\", \"decompose task\", \"subtasks\", \"dynamic adjustment\", \"reasoning\", \"tool-use capabilities\", \"generalization\", \"evaluation\", \"representative tools\", \"open problems\", \"integration\", \"future research\"]\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['foundation models']\n",
      "Few-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"foundation models\"]\n",
      "Self-consistency Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Zero-shot Accuracy: 0.8\n",
      "Few-shot Accuracy: 0.7\n",
      "Self-consistency Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "zero_shot_correct = 0\n",
    "fw_correct = 0\n",
    "self_consistency_correct = 0\n",
    "\n",
    "for i, eg in enumerate(val_data):\n",
    "    print(f\"Example {i}:\")\n",
    "    #print(\"Question: \" + eg['abstract'])\n",
    "    print(\"Real Answer: \" + str(eg['tags']))\n",
    "    print(\"----------------------\")\n",
    "    print(\"Zero-shot Predicted Answer: \" + zero_shot_predictions[i]['text'])\n",
    "    print(\"Zero-shot Predicted Grade: \" + zero_shot_graded_outputs[i]['results'])\n",
    "    print(\"----------------------\")\n",
    "    print(\"Few-shot Predicted Answer: \" + few_shot_predictions[i]['text'])\n",
    "    print(\"Few-shot Predicted Grade: \" + fw_graded_ouputs[i]['results'])\n",
    "    print(\"----------------------\")\n",
    "    print(\"Self-consistency Predicted Answer: \" + final_self_consistency_predictions[i]['text'])\n",
    "    print(\"Self-consistency Predicted Grade: \" + self_consistency_graded_outputs[i]['results'])\n",
    "    print(\"----------------------\")\n",
    "    print(\"\\n\")\n",
    "    # track the number of \"Correct\" grades for each model\n",
    "    if zero_shot_graded_outputs[i]['results'].strip(\" \") == \"CORRECT\":\n",
    "        zero_shot_correct += 1\n",
    "    if fw_graded_ouputs[i]['results'].strip(\" \") == \"CORRECT\":\n",
    "        fw_correct += 1\n",
    "    if self_consistency_graded_outputs[i]['results'].strip(\" \") == \"CORRECT\":\n",
    "        self_consistency_correct += 1\n",
    "\n",
    "print(\"Zero-shot Accuracy: \" + str(zero_shot_correct/len(val_data)))\n",
    "print(\"Few-shot Accuracy: \" + str(fw_correct/len(val_data)))\n",
    "print(\"Self-consistency Accuracy: \" + str(self_consistency_correct/len(val_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: LangChain's built-in grading prompt template is not optimal and we have developed a more optimal one for this use case in the course. As an exercise, try to integrate that prompt to the grader. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
